{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torchio as tio\n",
    "from models.networks import *\n",
    "from easydict import EasyDict\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary utility functions\n",
    "\n",
    "def get_data(data_path):\n",
    "    subjects = []\n",
    "    data = np.load(os.path.join(data_path))\n",
    "    # load data\n",
    "    t1, t2 = data[:1], data[1:2]\n",
    "    subject = tio.Subject(\n",
    "        t1 = tio.ScalarImage(tensor=t1),\n",
    "        t2 = tio.ScalarImage(tensor=t2)\n",
    "    )\n",
    "    subjects.append(subject)\n",
    "    dataset = tio.SubjectsDataset(subjects)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model\n",
    "\n",
    "with open('core/config.yaml') as f:\n",
    "    config = EasyDict(yaml.safe_load(f))\n",
    "\n",
    "# define model\n",
    "enc_s = Encoder(inc=config.dataset.nmodal, zdim=config.model.latent_dim).cuda()\n",
    "enc_s = nn.DataParallel(enc_s, device_ids=config.misc.devices).cuda()\n",
    "enc_c = Encoder(inc=config.dataset.nmodal, zdim=config.model.latent_dim).cuda()\n",
    "enc_c = nn.DataParallel(enc_c, device_ids=config.misc.devices).cuda()\n",
    "dec = Decoder(outc=config.dataset.nmodal, zdim=config.model.latent_dim).cuda()\n",
    "dec = nn.DataParallel(dec, device_ids=config.misc.devices).cuda()\n",
    "\n",
    "state_dict = torch.load('results/checkpoint.pth')\n",
    "enc_s.load_state_dict(state_dict['enc_s'])\n",
    "enc_c.load_state_dict(state_dict['enc_c'])\n",
    "dec.load_state_dict(state_dict['dec'])\n",
    "\n",
    "enc_s.requires_grad_(False)\n",
    "enc_c.requires_grad_(False)\n",
    "dec.requires_grad_(False)\n",
    "\n",
    "enc_s.eval()\n",
    "enc_c.eval()\n",
    "dec.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference\n",
    "\n",
    "config.dataset.src_dir = 'data/processed/6m'\n",
    "config.dataset.dst_dir = 'data/processed/12m'\n",
    "\n",
    "src_names = os.listdir(config.dataset.src_dir)\n",
    "dst_names = os.listdir(config.dataset.dst_dir)\n",
    "\n",
    "dst_name = random.sample(dst_names, 1)[0]\n",
    "for src_name in src_names:\n",
    "    src = get_data(os.path.join(config.dataset.src_dir, src_name))[0]\n",
    "    dst = get_data(os.path.join(config.dataset.dst_dir, dst_name))[0]\n",
    "    transform = tio.CropOrPad(src.shape[1:])\n",
    "    dst = transform(dst)\n",
    "    # define sampler and aggragator\n",
    "    sampler_src = tio.inference.GridSampler(src, config.model.patch_size, config.test.patch_overlap)\n",
    "    loader_src = DataLoader(sampler_src, config.test.batch_size)\n",
    "    sampler_dst = tio.inference.GridSampler(dst, config.model.patch_size, config.test.patch_overlap)\n",
    "    loader_dst = DataLoader(sampler_dst, config.test.batch_size)\n",
    "    aggregator = tio.inference.GridAggregator(sampler_src, 'average')\n",
    "    # extract patch\n",
    "    for i, (patch_src, patch_dst) in enumerate(zip(loader_src, loader_dst)):\n",
    "        src = torch.cat([patch_src['t1'][tio.DATA], patch_src['t2'][tio.DATA]], 1).cuda()\n",
    "        dst = torch.cat([patch_dst['t1'][tio.DATA], patch_dst['t2'][tio.DATA]], 1).cuda()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # style code\n",
    "            s_y = enc_s(dst)\n",
    "            # content code\n",
    "            c_x, emb_x = enc_c(src, True)\n",
    "            # transfer\n",
    "            syn_y = dec(c_x + s_y, emb_x)\n",
    "        # install\n",
    "        loc = patch_src[tio.LOCATION]\n",
    "        aggregator.add_batch(syn_y, loc)\n",
    "    print(i+1 == len(loader_src))\n",
    "    # syn is src content with dst style\n",
    "    syn = aggregator.get_output_tensor().cpu().numpy()\n",
    "    # concat segmentation\n",
    "    src = np.load(os.path.join(config.dataset.src_dir, src_name))\n",
    "    seg = src[-1:]\n",
    "    syn = syn * (seg > 0)\n",
    "    syn = np.concatenate([syn, seg])\n",
    "    # store\n",
    "    np.save(os.path.join(config.test.out_dir, os.path.basename(config.dataset.src_dir), src_name), syn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
